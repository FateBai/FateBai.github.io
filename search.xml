<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Transformer学习</title>
    <url>/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>2017年Google提出Transformer模型。过去了四年，想要入门Transformer原本是非常容易的，网上的资源一搜一大堆，但是大同小异，或者说没说到的地方都没说到，初学者看了之后除非悟性极好，否则还是不能理解(比如我)。所以我想尽量详细地叙述这个模型，综合网上各种贴子，可能你会有熟悉感。</p>
<p>修完大学公共数学基础三部曲即可。</p>
<h2 id="2-总体概述"><a href="#2-总体概述" class="headerlink" title="2 总体概述"></a>2 总体概述</h2><p>首先祭出这张最经典的论文图。</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-1.png" alt></p>
<p>总体上Transformer模型使用的是 解码器-译码器 的模式，即encoder-decoder。直观上讲，</p>
<p>就是一个输入，被输入到encoder模块当中，encoder模块输出一个中间产物，中间产物被decoder使用(应该是反复使用)，结合decoder本身的输入，经过一系列运算，输出结果(中间结果)。</p>
<p>理解上可以当做一个阅卷过程。encoder是试题组，综合考卷试题，给出一个给分细则，decoder像批卷老师，decoder输入是一份未批阅试卷，老师一手拿着给分细则打分，打分收到你之前题目作答情况，比如之前老师觉得给分太低，这时候有可能补偿式打分，最后将整分卷子批完。当然，给分一定合理吗？并不是。​</p>
<p>这是个大概理解，接下来才是重点。</p>
<h2 id="3-各模块分析"><a href="#3-各模块分析" class="headerlink" title="3. 各模块分析"></a>3. 各模块分析</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="1-input-embedding-输入嵌入"><a href="#1-input-embedding-输入嵌入" class="headerlink" title="1. input embedding (输入嵌入)"></a>1. input embedding (输入嵌入)</h4><p>以翻译为例。我要翻译一句话：I am a man. </p>
<p>我们中间是要对其进行数学运算，显然字符不合适，需要转化成数字。比如 数字1表示 I ，数字2表示 am，数字3表示 a , 数字4表示 man。只不过这是最朴素的想法，事实上一个句子中的每个单词，都有个<strong>词向量</strong>去表示，例如 man 可以表示成</p>
<script type="math/tex; mode=display">
Vetcor(man) = [0,0,0,1]</script><p>这个叫做$one-hot$编码方式，最简单的一种，直接“看式思义“。但是这个词向量长度维数非常高（不应该叫”长度“, “大小”感觉还可以），存储开销比较大，于是利用某些技术，降低维度，</p>
<script type="math/tex; mode=display">
Vector'(man) = [x_1,x_2,x_3]</script><p>某些技术指词嵌入技术，比如$Word2Vec$​​, 可以在本站搜索，没搜索到应该是我还没写。  = = </p>
<p>转化成向量，很多计算就更加方便了，可以牵扯到矩阵的运算。向量-&gt;矩阵。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>$x_{(0,0)}$​</td>
<td>$x_{(0,1)}$​</td>
<td>$x_{(0,2)}$​</td>
<td>$x_{(0,3)}$​</td>
<td>$x_{(0,4)}$​</td>
</tr>
<tr>
<td>am</td>
<td>$x_{(1,0)}$​</td>
<td>$x_{(1,1)}$​​</td>
<td>$x_{(1,2)}$​​</td>
<td>$x_{(1,3)}$​​</td>
<td>$x_{(1,4)}$​​</td>
</tr>
<tr>
<td>a</td>
<td>$x_{(2,0)}$​</td>
<td>$x_{(2,1)}$​</td>
<td>$x_{(2,2)}$​</td>
<td>$x_{(2,3)}$​</td>
<td>$x_{(2,4)}$​</td>
</tr>
<tr>
<td>man</td>
<td>$x_{(3,0)}$​</td>
<td>$x_{(3,1)}$​</td>
<td>$x_{(3,2)}$​</td>
<td>$x_{(3,3)}$​</td>
<td>$x_{(3,4)}$​</td>
</tr>
</tbody>
</table>
</div>
<p>这样一个句子就转化成了矩阵，每一行是一个单词的词向量。实际上词向量列数有很多，整个矩阵大小是$sequenceLength\ ×\ d_{model}$​​​​​​​​​​​​​​​​​​​​​ , 而真正的输入$X$​​​​​​​​​​​​​​​​​​​​，是很多个这样类似的矩阵，是一个 $batchSize\ ×\  sequenceLength\ ×\  d_{model}$​​​​​​​​​​​​​​​​​​​​​​​​  的张量。</p>
<p>$d_{model}$ 论文中大小采用512。</p>
<p>$batchSize$​​ 一般指同时代入训练模型的实例个数。因为你总不能把所有句子所代表的矩阵全扔进去。</p>
<h4 id="2-position-embedding-位置嵌入"><a href="#2-position-embedding-位置嵌入" class="headerlink" title="2. position embedding (位置嵌入)"></a>2. position embedding (位置嵌入)</h4><p>位置信息在翻译当中是重要的。</p>
<p>You do like it. (你确实喜欢它) Do you like it? (你喜欢它吗？) 翻译上存在不同。</p>
<p>position embedding 就是刻画位置信息的编码，类似于词向量。</p>
<p>分为绝对位置编码，三角式，训练式，相对位置编码等等。建议阅读，</p>
<p><a href="https://www.zhihu.com/search?type=content&amp;q=transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81">https://www.zhihu.com/search?type=content&amp;q=transformer%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81</a></p>
<p>论文当中采用三角式，</p>
<p>$PE(pos,2i) = sin(pos/10000^{2i/d_{model}})$</p>
<p>$PE(pos,2i+1) = cos(pos/10000^{2i/d_{model}})$​</p>
<p>$pos$​ 是单词在句子中的位置，$pos\in [0,sequenceLength)$​ , $i \in [0,d_{model})$​</p>
<p>而事实上，目前三角式用处比较小，相对位置编码更加重要，见</p>
<p><a href="https://mp.weixin.qq.com/s/vXYJKF9AViKnd0tbuhMWgQ">https://mp.weixin.qq.com/s/vXYJKF9AViKnd0tbuhMWgQ</a></p>
<p>最后信息添加的方式也非常简单，直接将输入矩阵$X = X+PE(X)$​​​​</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-2.png" alt></p>
<h4 id="3-Multi-Head-Attention-多头注意力机制"><a href="#3-Multi-Head-Attention-多头注意力机制" class="headerlink" title="3. Multi-Head Attention (多头注意力机制)"></a>3. Multi-Head Attention (多头注意力机制)</h4><p>这是核心部分。</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-3.png" alt></p>
<p>首先是$Q,K,V$​​。</p>
<p> 我们在之前得到了处理过的$X$​​了，我们需要用$X$​​得到$Q,K,V$​​​​​​. 转化如下（图中三个<em>Linear</em>部分)</p>
<script type="math/tex; mode=display">
Q = XW_Q,K = XW_K,V = XW_V</script><script type="math/tex; mode=display">
W_i\in{\R^{d_{model}×{d_{model}}}},i={Q,K,V}</script><p>$W_i$ 一般情况下，最初可以是个$d_{model}×d_{model}$ 的随机矩阵，要“学习”的内容也正是它，因此他的初值可以是随机的。那为什么要转化成三个不同矩阵呢？原因是为了将输入矩阵映射到不同的子空间，增强了表达能力，提高了泛化能力。</p>
<p>下面我们先看不进行分头处理的注意力机制，就是解释下述公式。</p>
<script type="math/tex; mode=display">
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>我们先看 $QK^T$ 是什么。</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-4.png" alt></p>
<p> $QK^T$ 是一个 $d_{model}×d_{model}$ 的<strong>注意力矩阵</strong>，每一个元素 $(QK^T)_{ij}$ 表示第 $i$ 个词和第 $j$ 个词的相联程度，而这种相联程度使用对应词向量的<strong>点积</strong>进行描述。</p>
<p>比如向量 $e_1,e_2$ 相似程度，我们可以用点积量描述，如图</p>
<p>$\vec{e_1}·\vec{e_2} = |\vec{e_1}||\vec{e_2}|cos&lt;\vec{e_1},\vec{e_2}&gt; = cos&lt;\vec{e_1},\vec{e_2}&gt;$​​ </p>
<p>在向量运算当中，  $e_1$ 比  $e_2$ 和 $e$ 的相似程度更高。 </p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-5.png" alt></p>
<p>那么为什么要除以 $\sqrt{d_k}$​ ？作用是把注意力矩阵变成标准正态分布，使得 $softmax$ 结果更加稳定。</p>
<p>那$softmax(x)$​ 是个怎样的函数呢？</p>
<blockquote>
<p><strong>Softmax函数</strong>，或称<strong>归一化指数函数</strong>，是逻辑函数的一种推广。它能将一个含任意实数的K维向量 <a href="javascript:"><img src="http://pic03.sogoucdn.com/s3/a/100520084/baike/formula/82eca5d0928078d5a61b9e7e98cc73db31070909.svg" alt="img"></a> “压缩”到另一个K维实向量 <a href="javascript:"><img src="http://pic04.sogoucdn.com/s3/a/100520084/baike/formula/2e610a6185b8850a6f567c4902387b17f0ec1652.svg" alt="img"></a> 中，使得每一个元素的范围都在<a href="javascript:"><img src="http://pic04.sogoucdn.com/s3/a/100520084/baike/formula/c79c6838e423c1ed3c7ea532a56dc9f9dae8290b.svg" alt="img"></a>之间，并且所有元素的和为1。           ——百度百科</p>
</blockquote>
<script type="math/tex; mode=display">
softmax(x_i) = \frac{e^{x_i}}{\sum^N_{j=1}e^{x_j}} , i=1,2,...,N</script><p>例如，$x = [1,2,3]$​ ,  $softmax(x) = [0.09003,0.24473,0.66524]$​​</p>
<p>可以发现 $softmax$ 函数将向量元素之和归一化到1，并且“放大”了元素之间的差值。</p>
<p>不过存在的问题就是指数运算过后，可能有上溢/下溢，解决方法就是对其进行变式。</p>
<p>经过这一系列处理，得到一个注意力矩阵，可以看作一个评分机制，或者是权值矩阵。我们再乘以 $V$ ，本质上是对 $V$​ 做一次求加权均值的过程。这样整个 $Attention(Q,K,V)$​ 就获得了句子整体的信息。</p>
<p>最后我们来解释多头的含义。</p>
<p>所谓多头，就是指将矩阵均分成 $h$ 组，每一组分别做注意力计算，最后我们再将他们连接到一起，再做一个线性变换，得到注意力层输出。需要注意的是 $h$ 需要能整除 $d_{model}$ 。</p>
<p> (论文中 $h$ 取8)</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-6.png" alt></p>
<p>我们说注意力机制是一种词和词之间的关系，一个词在每个头更“关注”的部分不同，使用多个头可以反映这种不同的关注，接合起来使得矩阵蕴含更复杂的信息。</p>
<h4 id="4-Add-amp-Norm-残差连接与归一化"><a href="#4-Add-amp-Norm-残差连接与归一化" class="headerlink" title="4. Add&amp;Norm (残差连接与归一化)"></a>4. Add&amp;Norm (残差连接与归一化)</h4><p>$Add$ 过程是一个残差连接的过程，做的事情就是 $Output_{Attention} = Output_{Attention} + X$  。这一处理主要目的是防止<strong>梯度消失</strong>。</p>
<p>$Norm$​ 过程是一个归一化的过程，主要目的是将矩阵按行化为标准正态分布，加快收敛过程，加快训练速度。</p>
<h4 id="5-Feed-Forward-前馈神经网络"><a href="#5-Feed-Forward-前馈神经网络" class="headerlink" title="5. Feed Forward (前馈神经网络)"></a>5. Feed Forward (前馈神经网络)</h4><script type="math/tex; mode=display">
Feedback(X) = W_1^T(ReLu(W_2^TX+b_2))+b_1</script><p>前馈神经网络主要作用是提供非线性转换，增强模型泛化能力。非线性部分指的是 $ReLu$​ 函数，常见的一种激活函数。</p>
<script type="math/tex; mode=display">
ReLu(x) = max\{0,x\}</script><p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-7.png" alt></p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><h4 id="6-Outputs-解码器输入"><a href="#6-Outputs-解码器输入" class="headerlink" title="6. Outputs (解码器输入)"></a>6. Outputs (解码器输入)</h4><p>解码器也是有输入的，输入为译码器输入句子的译文。这种译文输入形式类似译码器输入，并不是“翻译”结果，这个 $Outputs$ 是我们给定的。但是我们并不能让模型以一个“上帝视角”去学习，如果整个译文信息在翻译时被全部观测到，那“学习”是效果差的。所以需要 $mask$​ 技术使得翻译时不能够<strong>提前</strong>得到词的信息。</p>
<h4 id="7-Masked-Multi-Head-Attention"><a href="#7-Masked-Multi-Head-Attention" class="headerlink" title="7. Masked Multi-Head Attention"></a>7. Masked Multi-Head Attention</h4><p>$mask$ 分为两种。</p>
<p><img src="/2021/09/02/Transformer%E5%AD%A6%E4%B9%A0/ai-1-8.png" alt></p>
<p>其中一种 $mask$ 是一种填充 ($padding$) 技术，因为句子长度不一，我们并行处理的张量规模需要一致，因此我们选择其中最长的句子长度作为句子的尺度，空出来的部分（灰色），使用 0 填充。</p>
<p>第二种 $mask$ 只在 $decoder$​ 中使用，注意蓝色和橙色部分。我们不能利用未来的信息，所以也需要 $mask$​​ ，所利用的是之前的译文信息。使用 $-inf$ 填充。</p>
<h4 id="8-Linear-amp-amp-Softmax"><a href="#8-Linear-amp-amp-Softmax" class="headerlink" title="8. Linear &amp;&amp; Softmax"></a>8. Linear &amp;&amp; Softmax</h4><p>$Linear$​ 负责将得到的解码器输出映射到一个高维向量，维度取决于词典大小。</p>
<p>$Softmax$ 负责将这个向量转化为一个类似概率的输出，这样我们把概率大的词作为翻译后的词汇。</p>
<h2 id="3-过程"><a href="#3-过程" class="headerlink" title="3. 过程"></a>3. 过程</h2><p>例子 I am a man.</p>
<p>首先我们需要一个词典，记录用到的词。还有开始符(BOS)，结束符(EOS)，也被记录到词典里。</p>
<p>初始时，只有BOS一个符号。我们把句子以张量形式输入到译码器中，注意解码器和译码器并不是只有一个，而是有 $N$ 个复制。$Transformer$​ 的特点之一就是<strong>方便并行处理</strong>，提高效率。通过编码器我们得到一个隐藏层 (中间矩阵)，这时候我们利用这个编码器输出矩阵线性变换为解码器的 $K,V$ 输入，另外解码器还有输入部分就是给出的译文信息变换成的 $Q$ 。注意到编码器输出是要给到多个 $Decoder $ 的。每次翻译一个词，如下，</p>
<p>BOS -&gt; BOS 我 -&gt; BOS 我 是 -&gt; BOS 我 是 一 …… -&gt; BOS 我 是 一 个 男 人 EOS </p>
<p>我们定义损失函数，和真实翻译结果比较，运用反向传播算法，更新权值矩阵。</p>
<h2 id="4-结语"><a href="#4-结语" class="headerlink" title="4. 结语"></a>4. 结语</h2><p>考虑初学理解有限，有不对的地方欢迎指正，也请详细说说，谢谢！</p>
<p>2021/9/1 BRB， a Observer</p>
<h4 id><a href="#" class="headerlink" title=" "></a> </h4>]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>As Beginning</title>
    <url>/2021/08/14/As-a-beginning/</url>
    <content><![CDATA[<h2 id="应该叫作，一个开始"><a href="#应该叫作，一个开始" class="headerlink" title="应该叫作，一个开始"></a>应该叫作，一个开始</h2><p>应该先从blog的题目开始说起。As the bug of fate. 扑面而来的中二气息，可能你过了许久，看到这里仍然不太适应。很好，要的就是这种感觉，当你适应了，那更好，你是这个blog的挚友了。核心还是对于这个bug的理解。B-U-G。如果你对这个题目的理解是，super lucky man，那么显然和我不在一拍上。Bug 就是 bug。从主体意识诞生，脱离主体掌控，给予主体混沌。既然已经是命运的弃子，做一个fate的漏洞有什么不好？至少，我的坏运气来自我的无能，我的好运气，来自我的强大，我的所有抱怨，都将说给我自己。</p>
<p>这是一个测试，我不打算删除。</p>
]]></content>
  </entry>
  <entry>
    <title>无名之辈(起始篇)</title>
    <url>/2022/02/06/%E6%97%A0%E5%90%8D%E4%B9%8B%E8%BE%88/</url>
    <content><![CDATA[<p>一</p>
<p>“不是吧，兄弟，你已经是一个大学生了，不会真的有人还在上晚自习吧？”</p>
<p>林励并不是很想搭理吴风，但是过多的解释又会带来无休无止的闲谈，思考了片刻，林励决定以毒攻毒。</p>
<p>“不是吧，兄弟，不会还有人来这听听力吧？”</p>
<p>对，语气一定要恰到好处，攻守兼备，终结话题。</p>
<p>“瞧你说的，我这不是为了陪你？嗯？我怕你太孤独，太寂寞了呀！”</p>
<p>不对劲，平常一招就已经定胜负了，好在我还占据主动。林励想了想，一本正经地说道，</p>
<p>“没关系的，我一个人效率更高。”</p>
<p>这确实已经是一个大杀招了，几乎等于明示。</p>
<p>“好吧好吧，别说话了，影响我听力了，知道吗！”</p>
<p>终于安静下来了！这话术我还不曾输过谁。接下来，嗯，该数据机构了，我去，还有这么多题，林励看了看表，9点17分，还差1个小时关门！该加速加速了……什么动静，我耳鸣了吗？</p>
<p>“我说老吴，老吴，你有没有听见什么动静？”</p>
<p>“我不是很想和你说话，跟你说了，听力！听力！”，吴风其实也听到了一些动静，就和平时耳鸣一样。</p>
<p>吴风心想，难道我耳机戴时间太长了吗？这是不是在提醒我停止学习好好做人了？！转头看向林励，他好像又说些什么，可是，说了些什么呢，好像真的有些听不太清，不仅听不太清，甚至还有点看不清……</p>
<p>“老吴，老吴，吴……”林励现在只能听见一点点自己的声音，声音有些奇特，比平常更加有质感。</p>
<p>所以，现在到底发生了什么？</p>
<p>林励此时望向窗外，本该是夜晚的校园明如白昼，不，是白昼在从远方扩散而来，渐渐地淹没了一些…….</p>
<p>林励从笔袋里拿出了那个一元硬币，那枚从中学带到大学的硬币，每次遇到重要的决定，每次在优柔寡断之间快要窒息的时候，他会选择相信它，那0.5。</p>
<p>好吧，这次你告诉我，正面，我能活下来吗！他高高的将硬币抛弃，硬币翻转着上升，正面反面交替面朝天空，笔直的直线冲向天空。什么呀，这里可是二楼啊，不是天台啊，为什么能看到天啊！硬币还在翻转，不过已经看不清那个硬币了，渐渐地，它淹没在白光之中，林励到最后也没能看清，落下来的究竟是什么……</p>
<p>“E067”一个男性的声音传来。</p>
<p>E067？对了，我是E067。</p>
<p>不对，我不是E067。</p>
<p>不是E067我是什么呢。</p>
<p>我是……，我记得我有名字，应该不是E067。</p>
<p>我不想死，我不是E067。</p>
<p>我是谁呢。</p>
<p>我…，应该叫，林…</p>
<p>林励！这是哪！林励开始有了意识，眼睛渐渐睁开，眼前的一切让他以为他在做梦。</p>
<p>他躺在床上，床在屋子里，如果仅仅考虑这些概念好像是现实，但，这床压根就是一块木板，这屋子分明像牛棚！意识渐渐恢复了正常，但是慌乱却没有平复。</p>
<p>这是哪？我被拐卖到山区了吗是这样吗？林励马上下了床，环顾四周，没有人看守，又不太像被拐卖了，再说了哪有拐卖大学生的。</p>
<p>卧槽，不会是器官贩卖吧！</p>
<p>林励摸了摸身上，似乎也没什么太大变化，甚至衣服，包括衣服里的东西都还在，居然还有那个一元硬币！</p>
<p>不过周围的环境贫苦得厉害，这不就是电视里最最最偏远山区的样子吗！阳光居然能透过屋顶照在土地上，是真正的“土地”，周围除了大大小小的水缸外，只有一个铃铛挂在了床头。</p>
<p>他并没有去随意触碰这个铃铛，而是小心翼翼地推开了门，一个男人站在门前，似乎有些惊讶。</p>
<p>“你醒了，E067。”声音有些低沉。</p>
<p>这个全身都是灰色的男人胸前印着一串字符。这一定是某种标记，林励想到了当时脑海当中的声音，和这个男人有几分相似。</p>
<p>“嗯，我醒了，E066。”</p>
<p>那个男人似乎非常满意，甚至开怀大笑，</p>
<p>“我知道你死不了的。”</p>
<p>“嗯，也许吧。”</p>
<p>二</p>
<p>林励来到这里已经三天了，这几天他也渐渐了解了一些东西，关于这里的东西。他已经有空开始整理这些信息了。</p>
<p>首先，这里的人以“代号”相称呼，比如他就是E067，那个男人就是E066。似乎周围的这些人都是E+数字组成的字符，不过不知道还有没有其他字母，字母一定有某种含义在里面。</p>
<p>其次，这就是山区。是更加落后，贫苦的山区，没有任何电的产品，更加倾向于深山老林一样的感觉。几次他都想走出去，可是这里的山路像迷宫一样，与其被困在荒无人烟的地方，还不如在多准备准备再出发。</p>
<p>再者，自己的穿着居然在这里引起了不小的关注，显然这里其他每一个人穿着都是灰色的衣服裤子，无一例外。本来自己的穿着挺保守，怕在学校引起什么注意，现在竟然成了奇葩一个。不过其实这里的人还好，尤其是那个E066，对他还是很照顾的，一直听他说自己是他的好兄弟，至于原因，好像就是E066和E067从前在一个地方打工，互相照顾。这么说来，打工？也就是他知道有城镇的存在喽？“那么这是M市吗？”每次自己这么问，他总是摇摇头，每次都是。那么，这里是中国吗？他又摇了摇头。我甚至不明白他是不知道，还是不知道。语言既然相同，这里不是中国是哪？再问下去也是白搭，还不如自己找答案。</p>
<p>现在为止，最最重要的事情就是，离开这里。</p>
<p>林励也不想引起注意，想从男人那里要一件一样的衣服，得到的答复却是，</p>
<p>“一人一件，你的那件在这。”</p>
<p>男人递过来一件仔细叠过的衣服，不过补丁还是非常明显。</p>
<p>“这难道不一样吗？”</p>
<p>男人摇了摇头。</p>
<p>“有什么不一样呢？哦，我明白了。”</p>
<p>林励笑了。自己已经是E067了。</p>
<p>有一说一，这里的茶还是不错，不过为什么说不错呢？其实林励也不怎么会品茶，但喝了之后，总会有种莫名的舒适感。</p>
<p>可能是，我已经开始适应了吧？他这样想。</p>
<p>经过了几天观察，发现自己在的地方就是一个自给自足的深山老林，像是一个小的村落，虽然这个结论早早就已经下过了，但是将这个结论确定是多么令人痛苦。</p>
<p>不行，我不能一直待着！眼下唯一的希望就是从这个E066身上获得一些信息。</p>
<p>“挑水呢兄弟！”</p>
<p>男人冲着林励笑了笑，却没有说话，自个挑水走进屋里。</p>
<p>什么啊，这么腼腆？不对，这，感觉就是木讷！冲我笑干嘛？我认识你吗！</p>
<p>可能是这几天的等待让林励有些烦躁。</p>
<p>他直接冲进屋里，质问道，</p>
<p>“告诉我，兄弟，告诉我你能告诉我的所有！兄弟，不然我在这里待不下去的！”</p>
<p>他从那个补了两个补丁的口袋里掏出了一张叠好的纸，似乎是什么重要的信。</p>
<p>“看完它，还给我。”</p>
<p>“…”</p>
<p>看来这确实是什么重要的信，给谁的呢？林励打开了信，男人却急急忙忙地出了屋子。</p>
<p>“这！这…”</p>
<p>林励将信重新叠好，还给了男人。</p>
<p>不敢相信眼前的一切，这个信里的故事真的是匪夷所思。看来这个已经不是我这个相信科学的人可以理解的事情了。不过，实践才是真理，这，总是全宇宙通用的吧。林励苦笑了许久，终于，渐渐适应了。</p>
<p>“走，收拾一下，明天就去你，哦不，就去我们打工的地方。”</p>
<p>数十米古围墙绵延数里，入城的车马络绎不绝。墙外是数十平方千米的平原，然后才是绵延的山峦。不过最奇特的，要数那比城墙还高的龙首雕像，不知道那里到底是什么神奇的地方，反正，一定很高吧。</p>
<p>虽然林励在网络上见到过高它几倍的大楼，但是这种压迫感，是远远不够的。</p>
<p>林励刻意地让E066走在前面，毕竟自己又不识路，但是又要时刻装得驾轻就熟，这就非常难受了。花了整整一天的时间才从半山腰（可能是）下来，已经坚持了一天了，还需要时刻面对E066那“天真无邪”的笑脸，回复一个不失礼貌的微笑，真的好累啊！</p>
<p>所以，为什么，为什么我要为了别人活着？</p>
<p>别人，别人算什么啊！</p>
<p>可是，我做不到啊！</p>
<p>我，为什么没有拒绝呢！</p>
<p>我有拒绝这个选项吗？没有！</p>
<p>“到了。”男人再次露出了笑容。</p>
<p>林励再一次回以礼貌地微笑。</p>
<p>“你和以前一样爱笑。”这句话让林励猝不及防，原来，他也会说点有人味的话啊，还以为你不会呢。</p>
<p>“这是？”</p>
<p>“餐馆。”</p>
<p>“不用了，我还不是很饿。”林励是真的有点怕麻烦。</p>
<p>“不是，我们在这打工。”</p>
<p>什么！</p>
<p>“这个玻璃怎么擦的！你看没看到啊，干了之后更花了！”</p>
<p>“这，没办法，怎么擦都会有的啊。”</p>
<p>“什么？你看看E066怎么擦的，好好学学，怎么这么久不见，干活都不会干了！”店主（D101）有些生气地说道。</p>
<p>林励看向E066，只见他左手拿着一块抹布，右手也拿着一块抹布……</p>
<p>林励恍然大悟。我的天，我懂了，一块干的一块湿的，我去，以前家里好像就是这么擦的，我怎么给忘了！不过，这也太麻烦了点，也不知道工资多少啊。</p>
<p>那该死的太阳总算下山了，第一天的打工日子有些不太顺利地告一段落了。</p>
<p>一个餐馆旁边偏僻的小楼，林励，E067，躺在了一张对于脊椎不太好的人很是合适的硬板床上，当然，除了对脊椎好之外，简直就是灾难。不过比起山上屋子里的裸木板，至少还有些布制品包裹，有些地方，透过布料，还是能看出棕色木头的样子。</p>
<p>旁边，另一张床上，就躺着E066。</p>
<p>林励反复在脑海中思索着事情。</p>
<p>你，原本的你，究竟为什么，要做到这个地步？</p>
<p>我，无辜的我，究竟为什么，要走这条路，甚至你都没告诉我方向在哪。</p>
<p>“我说，兄弟，你们这打工……，不是，我们在这打工……，我的意思是，最近这个工资有没有涨钱啊，还是之前那个样吗？”</p>
<p>“周结，0.5”</p>
<p>“说完了吗，0.5单位是什么呢？”</p>
<p>“单位？”</p>
<p>看到E066那疑惑得不行的表情，林励忽然明白了一件事情，这里不能用现代社会的尝试去理解事物，不如当做穿越到异世界那种理解模式。</p>
<p>“没事没事，我可能有点忘记了。”</p>
<p>这个理由确实是万能的。</p>
<p>“大多数人的钱，是没有单位的。”</p>
<p>大多数人？不是全部的？</p>
<p>“那……”</p>
<p>林励刚想会问过去，发现E066已经将头转过去了。这时候，确实不应该再问下去了。</p>
<p>确实，这几天店里面管吃管住，自己没有任何消费，下意识以为这里是一样的。没想到……</p>
<p>这有限的对话时间真的是难得，明天早上又要7点起，晚8点退。真是难得这里居然有时间观念，不过店里面像样的钟表也并没有看到，每次只是听到店主喊到这些，真的挺离谱的。</p>
<p>林励此时有了困意，但不是很强烈。以前经常的，只要晚上在想些事情，无论白天多么累，大脑始终会处于一种活跃的状态来击退困意，不过下一天早上会格外得累。</p>
<p>再过几天，我熟悉了这里，还有件重要的事情要办。</p>
<p>真有你的，E067。</p>
<p>工作的六天终于过去了，打工，从窗户到桌子，甚至到厨房，都要涉及。每个打工人，都是六边形战士。</p>
<p>“该付你们工资了。”又是那个长头发男人——店主，D101。</p>
<p>只见D101拿出了一个笔，金属光泽，在林励和E066胸前的牌子上沿着标号描了一下，笔的末端还散发着红色的光。</p>
<p>这应该，是科技吧。林励心想。怪不得没有单位，这种是类似于点数一样的东西，对吧？</p>
<p>林励回想起在餐馆见到过有人被一个笔描过胸前的序号，但是那时，笔的末端散发着绿色的光，是不是意味着那时花钱呢？</p>
<p>“行了，你们休息去吧。”店主轻声说道。</p>
<p>“好好，谢谢您。”林励脱口而出。</p>
<p>“谢谢。”E066笑了笑，转头走开了。</p>
<p>那么接下来，还有个很重要的事情要处理！林励笑了笑，转头跟上，</p>
<p>“并且这件事，还不能跟你说，诶。”</p>
<p>三</p>
<p>龙颚之塔，这个城市最高的建筑，笔直地伫立在中心市区的圆形广场上，塔的四周由近及远分别是圆环河道，圆环草坪，圆环石子路。龙首在塔的顶部，守望着东方，每天迎着太阳，目视着太阳，气势让人生畏。</p>
<p>这里的建筑或者说一切，有一种现代和古代的结合，一种说不清的暧昧。金属泛着特有的光芒，木材制品却随处可见；玻璃在一些店面大量使用，纸糊的窗户也不在少数；胸前序号扫描技术都已经存在，电子产品却无影无踪。种种这些，让人极其难以辨别它的年龄，岁月的年龄。所以，到底是什么，造就了这些，时间的错位？即便是普通常见的事物，这里的运行仍然让人感到不自然，仿佛有着什么，在欺骗着规则。</p>
<p>即便是一个城市内，也有着双重的结界分离着不同的人们，这便是内城墙。和数十米的外城墙相比，七八米的内城墙显得矮小不少，但是显得更加重要。四个入口控制着进出的人群，每个入口有三名士兵把守着三米左右的拱门，来往的车马也不是少数。</p>
<p>林励独自一个人，来到了这里。</p>
<p>“队长，你看这人”，一个看起来三十岁左右的士兵朝着他旁边的士兵说道，“是E。”</p>
<p>“E？不会又是新生吧，去问问。”</p>
<p>“小子，来内城做什么？”被叫做“队长”的那个人朝林励喊道。</p>
<p>“参观……不是，我是找人。”</p>
<p>“找人？不是去龙颚序啊？”</p>
<p>“龙颚序？那是哪？”林励也确实没有从任何地方得知这个地方的名字。</p>
<p>“行了，不是去那的。该怎么办怎么办吧。”</p>
<p>“队长”示意了他旁边的士兵，那个士兵气势汹汹地走了上去，</p>
<p>“E067。离开这！”士兵已经准备将林励给架走，</p>
<p>“我是找人的，真的是。”</p>
<p>“无非是想进内城是吗？E066。”他小声地对林励说道。</p>
<p>“对对，我很想进去。”</p>
<p>“那你真的是在做梦，给我滚！”声音大的可怕，几乎所有人都听到了这句话。</p>
<p>当然，几乎所有人都露出了鄙夷的眼光，只有几个女人露出了该死的怜悯。</p>
<p>这里的人，真的是太不友好了。</p>
<p>林励朝士兵奉承地笑了笑，</p>
<p>“兄弟，大哥，我找D059。”林励瞥见了那个士兵手腕上的序号，D179。</p>
<p>“D059？”</p>
<p>另一士兵也走了过来，</p>
<p>“查一查D059。”</p>
<p>D179查了一番过后，整理了一下自己的帽子，</p>
<p>“队长，D059是一名龙颚序的教师。”</p>
<p>那个“队长”似乎有些惊愕，但很快的恢复了平静，然后又露出一副好像之前的事情从来没有发生过一样的表情，</p>
<p>“准许通过！”</p>
<p>不过随后的表情却有些微妙，那似乎是嘲讽似的笑容，终于露了出来。</p>
<p>D179？林励边走着，边在回忆。果然是社会等级制度，这落后的地方，刁民！有些生气，但是他无处可发泄。已经多久，没有受到过这种区别对待了，林励已经快忘记，如何对抗这种人了。</p>
<p>“你来了？”一个皮肤黝黑的光头男子转头看向林励，额头上一道水平的疤痕触目惊心。中年的样子，青年般的强健体魄。</p>
<p>“你好。”林励仔细打量着眼前这个男人，要比一米八的自己矮了半个头，但是差不多等于一个半自己的宽度！整个手臂像一般人大腿的粗细，青筋暴起，皮肤似乎和床板一个颜色，加上那个骇人的伤疤，整个形象活脱像一个悍匪，一个训练有素的悍匪一般。没想到这种人，居然在学校任职！也是，人不可貌相，也许就是从战场上归来的老兵改行训练了罢了。</p>
<p>“里面说。”男人的语气十分客气，举止十分有礼，这让林励有些不适。</p>
<p>进了一个教室，教室里空无一人。</p>
<p>“那个……”</p>
<p>林励其实也没想好如何开口，毕竟是个很玄乎的事情。</p>
<p>他在等着他。</p>
<p>林励知道，这个男人在确定某种东西，犀利的目光无时无刻不在像自己袭来。</p>
<p>“你的老朋友失忆了。”</p>
<p>“什么？”</p>
<p>林励很奇怪这人的听力会有些问题，再次强调道，</p>
<p>“你的老朋友失忆了。”</p>
<p>没错啊，信上就让我这么说的，一个字都没有问题。林励心想。</p>
<p>男人目光中有一丝游离，但很快恢复了锐利。</p>
<p>“没关系的，我会帮你恢复记忆的。在这个过程当中，你要到龙颚序来上学。”</p>
<p>“我能不能问一下，其中的原因，毕竟我也记不起什么了。”林励还不能确定，眼前的这个人是否真的值得信任，或者说，E067之前有跟他说过什么吗？</p>
<p>“让自己变强，才能生存的更久，这也是你以前经常说的。”男人将自己的左手伸出，手腕上印有黑色的序号，D059，</p>
<p>“你还需要了解更多，老朋友。”</p>
<p>有一刻，林励有些惊讶，来源于对于序号这种未知科技的吃惊，D,E,手腕，胸口，这确实有某种逻辑上的递进，还有一些科学上的渐进。</p>
<p>难道，这是一种魔法吗？对了，这是不是真实的科学的世界还是有待商榷的事情啊！</p>
<p>林励发现，事情的发展在朝着不可理解的方向前进。</p>
<p>没关系，我会适应这里的，以前也是，现在，也是。</p>
<p>林励心里波澜乍起。</p>
]]></content>
      <categories>
        <category>小说</category>
      </categories>
  </entry>
</search>
